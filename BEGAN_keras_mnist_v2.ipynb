{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BEGAN_keras_mnist.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GTU7Tt50oAUu"},"source":["# BEGAN을 이용한 이미지생성"]},{"cell_type":"markdown","metadata":{"id":"KrNMDJkEsdnZ","colab_type":"text"},"source":["라이브러리 읽어들이기"]},{"cell_type":"code","metadata":{"id":"RwGecgAnpGPV","colab_type":"code","colab":{}},"source":["import os\n","\n","import numpy as np\n","from tensorflow.python import keras\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras import losses\n","from tensorflow.python.keras.optimizers import Adam\n","from tensorflow.python.keras.models import Sequential, Model\n","from tensorflow.python.keras.layers import Conv2D, Conv2DTranspose, Activation, Flatten, Dense, UpSampling2D, Reshape, Lambda, Input\n","from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.python.keras.preprocessing.image import img_to_array, array_to_img"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eHgnAUROsf8m","colab_type":"text"},"source":["구글 드라이브 연동하기"]},{"cell_type":"code","metadata":{"id":"GxvN8vw6Br9z","colab_type":"code","outputId":"fe20eada-0469-49d8-c182-3c23b94374f0","executionInfo":{"status":"ok","timestamp":1576114816636,"user_tz":-540,"elapsed":1156,"user":{"displayName":"김성신","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBDkcdDRtjexFDpgc5dOd19_nhgffecissjauBQWg=s64","userId":"05257912891226351338"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":215,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"57nBisFgszfP","colab_type":"text"},"source":["이미지를 저장하는 함수"]},{"cell_type":"code","metadata":{"id":"ubWsBcpoNIib","colab_type":"code","colab":{}},"source":["def save_imgs(path, imgs, rows, cols):\n","    \"\"\"이미지를 타일 형태로 저장\n","    \n","    Arguments:\n","        path (str): 저장할 폴더 경로\n","        imgs (np.array): 저장할 이미지 리스트\n","        rows (int): 타일의 세로 크기\n","        cols (int): 타일의 가로 크기\n","        \n","    Returns:\n","        None\n","    \"\"\"\n","    base_width = imgs.shape[1]\n","    base_height = imgs.shape[2]\n","    channels = imgs.shape[3]\n","    output_shape = (\n","        base_height*rows,\n","        base_width*cols,\n","        channels\n","    )\n","    buffer = np.zeros(output_shape)\n","    for row in range(rows):\n","        for col in range(cols):\n","            img = imgs[row*cols + col]\n","            buffer[\n","                row*base_height:(row + 1)*base_height,\n","                col*base_width:(col + 1)*base_width\n","            ] = img\n","    array_to_img(buffer).save(path)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g8UhYjeCtT4_","colab_type":"text"},"source":["이미지 데이터 읽어 들이기"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1576116130638,"user_tz":-540,"elapsed":921,"user":{"displayName":"김성신","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBDkcdDRtjexFDpgc5dOd19_nhgffecissjauBQWg=s64","userId":"05257912891226351338"}},"id":"HM6HhYisP1QS","outputId":"526bbd7c-73a6-4cb5-f59c-27b5801f0231","colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["DATA_DIR = '/content/drive/My Drive/data/'\n","\n","BATCH_SIZE = 16\n","IMG_SHAPE = (28, 28, 3)\n","\n","data_gen = ImageDataGenerator(rescale=1/255.)\n","train_data_generator = data_gen.flow_from_directory(\n","    directory=DATA_DIR,\n","    classes=['mnist'],\n","    class_mode=None,\n","    batch_size=BATCH_SIZE,\n","    target_size=IMG_SHAPE[:2]\n",")"],"execution_count":265,"outputs":[{"output_type":"stream","text":["Found 240 images belonging to 1 classes.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ckxw89vJtXfe","colab_type":"text"},"source":["Encoder 정의"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yubnwEo77Z0l","colab":{}},"source":["def build_encoder(input_shape, z_size, n_filters, n_layers):\n","    \"\"\"Encoder구축\n","    \n","    Arguments:\n","        input_shape (int): 이미지의 shape\n","        z_size (int): 특징 공간의 차원 수\n","        n_filters (int): 파일 수\n","        \n","    Returns:\n","        model (Model): 인코더 모델 \n","    \"\"\"\n","    model = Sequential()\n","    model.add(\n","        Conv2D(\n","            n_filters,\n","            3,\n","            activation='elu',\n","            input_shape=input_shape,\n","            padding='same'\n","        )\n","    )\n","    model.add(Conv2D(n_filters, 3, padding='same'))\n","    for i in range(2, n_layers + 1):\n","        model.add(\n","            Conv2D(\n","                i*n_filters,\n","                3,\n","                activation='elu',\n","                padding='same'\n","            )\n","        )\n","        model.add(\n","                Conv2D(\n","                i*n_filters,\n","                3,\n","                activation='elu',\n","                strides=2,\n","                padding='same'\n","            )\n","        )\n","    model.add(Conv2D(n_layers*n_filters, 3, padding='same'))\n","    model.add(Flatten())\n","    model.add(Dense(z_size))\n","    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PIZ-LGUTpcfm","colab_type":"text"},"source":["생성자/Decoder 정의"]},{"cell_type":"code","metadata":{"id":"eQtdcSemUDg8","colab_type":"code","colab":{}},"source":["def build_decoder(output_shape, z_size, n_filters, n_layers):\n","    \"\"\"Decoder 구축\n","    \n","    Arguments:\n","        output_shape (np.array): 이미지 shape\n","        z_size (int): 특징 공간의 차원 수\n","        n_filters (int): 파일 수\n","        n_layers (int): 레이어 수\n","        \n","    Returns:\n","        model (Model): 디코더 모델 \n","    \"\"\"\n","    # UpSampling2D로 몇 배로 확대할지 계산\n","    scale = 2**(n_layers - 1)\n","    # 합성곱층의 처음 입력 사이즈를 scale로부터 역산\n","    fc_shape = (\n","        output_shape[0]//scale,\n","        output_shape[1]//scale,\n","        n_filters\n","    )\n","    # 완전연결 계층에서 필요한 사이즈를 역산\n","    fc_size = fc_shape[0]*fc_shape[1]*fc_shape[2]\n","    \n","    model = Sequential()\n","    # 완전연결 계층\n","    model.add(Dense(fc_size, input_shape=(z_size,)))\n","    model.add(Reshape(fc_shape))\n","    \n","    # 합성곱층 반복\n","    for i in range(n_layers - 1):\n","        model.add(\n","            Conv2D(\n","                n_filters,\n","                3,\n","                activation='elu',\n","                padding='same'\n","            )\n","        )\n","        model.add(\n","            Conv2D(\n","                n_filters,\n","                3,\n","                activation='elu',\n","                padding='same'\n","            )\n","        )\n","        model.add(UpSampling2D())\n","        \n","    # 마지막 층은 UpSampling2D가 불필요\n","    model.add(\n","        Conv2D(\n","            n_filters,\n","            3,\n","            activation='elu',\n","            padding='same'\n","        )\n","    )\n","    model.add(\n","        Conv2D(\n","            n_filters,\n","            3,\n","            activation='elu',\n","            padding='same'\n","        )\n","    )\n","    # 출력층에서는 3채널로\n","    model.add(Conv2D(3, 3, padding='same'))\n","    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"62pqShRxqr3C","colab_type":"text"},"source":["\n","생성자 정의"]},{"cell_type":"code","metadata":{"id":"C9WoQlbDUHWa","colab_type":"code","colab":{}},"source":["def build_generator(img_shape, z_size, n_filters, n_layers):\n","    decoder = build_decoder(\n","        img_shape, z_size, n_filters, n_layers\n","    )\n","    return decoder"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GWmR4YIUqO_a","colab_type":"text"},"source":["구분자 정의"]},{"cell_type":"code","metadata":{"id":"BEjyU_G7UJ65","colab_type":"code","colab":{}},"source":["def build_discriminator(img_shape, z_size, n_filters, n_layers):\n","    encoder = build_encoder(\n","        img_shape, z_size, n_filters, n_layers\n","    )\n","    decoder = build_decoder(\n","        img_shape, z_size, n_filters, n_layers\n","    )\n","    return keras.models.Sequential((encoder, decoder))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0tUlFHFcrbWf","colab_type":"text"},"source":["구분자의 학습용 네트워크"]},{"cell_type":"code","metadata":{"id":"bDCKW3QMURHz","colab_type":"code","colab":{}},"source":["def build_discriminator_trainer(discriminator):\n","    img_shape = discriminator.input_shape[1:]\n","    real_inputs = Input(img_shape)\n","    fake_inputs = Input(img_shape)\n","    real_outputs = discriminator(real_inputs)\n","    fake_outputs = discriminator(fake_inputs)\n","\n","    return Model(\n","        inputs=[real_inputs, fake_inputs],\n","        outputs=[real_outputs, fake_outputs]\n","    )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZEOgJ0gxuTX1","colab_type":"text"},"source":["네트워크 구축"]},{"cell_type":"code","metadata":{"id":"oeyQKz3nuUjM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"1415c427-172c-46c5-b5de-ab731ff023b1","executionInfo":{"status":"ok","timestamp":1576116155869,"user_tz":-540,"elapsed":1272,"user":{"displayName":"김성신","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBDkcdDRtjexFDpgc5dOd19_nhgffecissjauBQWg=s64","userId":"05257912891226351338"}}},"source":["n_filters = 64  #  필터 수\n","n_layers = 3 # 레이어 수\n","z_size = 32  #  특징 공간의 차원\n","\n","generator = build_generator(\n","    IMG_SHAPE, z_size, n_filters, n_layers\n",")\n","discriminator = build_discriminator(\n","    IMG_SHAPE, z_size, n_filters, n_layers\n",")\n","discriminator_trainer = build_discriminator_trainer(\n","    discriminator\n",")\n","\n","generator.summary()\n","# discriminator.layers[1]은 디코더를 나타냄\n","discriminator.layers[1].summary()"],"execution_count":271,"outputs":[{"output_type":"stream","text":["Model: \"sequential_56\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_48 (Dense)             (None, 3136)              103488    \n","_________________________________________________________________\n","reshape_34 (Reshape)         (None, 7, 7, 64)          0         \n","_________________________________________________________________\n","conv2d_324 (Conv2D)          (None, 7, 7, 64)          36928     \n","_________________________________________________________________\n","conv2d_325 (Conv2D)          (None, 7, 7, 64)          36928     \n","_________________________________________________________________\n","up_sampling2d_64 (UpSampling (None, 14, 14, 64)        0         \n","_________________________________________________________________\n","conv2d_326 (Conv2D)          (None, 14, 14, 64)        36928     \n","_________________________________________________________________\n","conv2d_327 (Conv2D)          (None, 14, 14, 64)        36928     \n","_________________________________________________________________\n","up_sampling2d_65 (UpSampling (None, 28, 28, 64)        0         \n","_________________________________________________________________\n","conv2d_328 (Conv2D)          (None, 28, 28, 64)        36928     \n","_________________________________________________________________\n","conv2d_329 (Conv2D)          (None, 28, 28, 64)        36928     \n","_________________________________________________________________\n","conv2d_330 (Conv2D)          (None, 28, 28, 3)         1731      \n","=================================================================\n","Total params: 326,787\n","Trainable params: 326,787\n","Non-trainable params: 0\n","_________________________________________________________________\n","Model: \"sequential_58\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_50 (Dense)             (None, 3136)              103488    \n","_________________________________________________________________\n","reshape_35 (Reshape)         (None, 7, 7, 64)          0         \n","_________________________________________________________________\n","conv2d_338 (Conv2D)          (None, 7, 7, 64)          36928     \n","_________________________________________________________________\n","conv2d_339 (Conv2D)          (None, 7, 7, 64)          36928     \n","_________________________________________________________________\n","up_sampling2d_66 (UpSampling (None, 14, 14, 64)        0         \n","_________________________________________________________________\n","conv2d_340 (Conv2D)          (None, 14, 14, 64)        36928     \n","_________________________________________________________________\n","conv2d_341 (Conv2D)          (None, 14, 14, 64)        36928     \n","_________________________________________________________________\n","up_sampling2d_67 (UpSampling (None, 28, 28, 64)        0         \n","_________________________________________________________________\n","conv2d_342 (Conv2D)          (None, 28, 28, 64)        36928     \n","_________________________________________________________________\n","conv2d_343 (Conv2D)          (None, 28, 28, 64)        36928     \n","_________________________________________________________________\n","conv2d_344 (Conv2D)          (None, 28, 28, 3)         1731      \n","=================================================================\n","Total params: 326,787\n","Trainable params: 326,787\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"swBw2e0GrwwK","colab_type":"text"},"source":["손실(loss) 함수 정의"]},{"cell_type":"code","metadata":{"id":"nMKo8Nz7UXmk","colab_type":"code","colab":{}},"source":["from tensorflow.python.keras.losses import mean_absolute_error\n","\n","def build_generator_loss(discriminator):\n","    # discriminator를 사용해서 손실 함수 정의\n","    def loss(y_true, y_pred):\n","        # y_true \n","        reconst = discriminator(y_pred)\n","        return mean_absolute_error(\n","            reconst,\n","            y_pred\n","        )\n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6CWx0rtDr0tU","colab_type":"text"},"source":["generator 컴파일"]},{"cell_type":"code","metadata":{"id":"8UGijY_BUaxT","colab_type":"code","colab":{}},"source":["# 초기 학습률(Generator)\n","g_lr = 0.0001\n","\n","generator_loss = build_generator_loss(discriminator)\n","generator.compile(\n","    loss=generator_loss,\n","    optimizer=Adam(g_lr)\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D7J0c9Owr4DC","colab_type":"text"},"source":["\n","discriminator 컴파일"]},{"cell_type":"code","metadata":{"id":"pRpY820HZRCg","colab_type":"code","colab":{}},"source":["# 초기 학습률(Discriminator)\n","d_lr = 0.0001\n","\n","# k_var는 수치(일반 변수)\n","k_var = 0.0\n","# k : Keras(TensorFlow) Variable\n","k = K.variable(k_var)\n","discriminator_trainer.compile(\n","    loss=[\n","        mean_absolute_error,\n","        mean_absolute_error\n","    ],\n","    loss_weights=[1., -k],\n","    optimizer=Adam(d_lr)\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lgZMurL_sCIC","colab_type":"text"},"source":["수렴 판정용 함수 정의"]},{"cell_type":"code","metadata":{"id":"I-R5uIGhsIKJ","colab_type":"code","colab":{}},"source":["def measure(real_loss, fake_loss, gamma):\n","    return real_loss + np.abs(gamma*real_loss - fake_loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bpIuIlQBsMaT","colab_type":"text"},"source":["학습 코드"]},{"cell_type":"code","metadata":{"id":"cwMoUzxiZiY4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":260},"outputId":"a46bd3dd-58a2-4bd7-a810-98f45c9e06d2"},"source":["# k의 갱신에 이용할 파라미터\n","GAMMA = 0.5\n","kLambda = 0.001\n","\n","# 반복 수. 100000～1000000 정도로 지정\n","TOTAL_STEPS = 10000\n","\n","# 모델과 확인용 생성 이미지를 저장할 폴더\n","\n","IMG_SAVE_DIR = '/content/drive/My Drive/data/imgs'\n","# 확인용으로 5x5 개의 이미지를 생성\n","IMG_SAMPLE_SHAPE = (5, 5)\n","N_IMG_SAMPLES = np.prod(IMG_SAMPLE_SHAPE)\n","\n","\n","# 저장할 폴더가 없다면 생성\n","os.makedirs(IMG_SAVE_DIR, exist_ok=True)\n","\n","# 샘플이미지용 랜덤 시드\n","sample_seeds = np.random.uniform(\n","    -1, 1, (N_IMG_SAMPLES, z_size)\n",")\n","\n","history = []\n","logs = []\n","\n","for step, batch in enumerate(train_data_generator):\n","  \n","\n","    # 임의의 값(noise) 생성\n","    z_g = np.random.uniform(\n","        -1, 1, (BATCH_SIZE, z_size)\n","    )\n","    z_d = np.random.uniform(\n","        -1, 1, (BATCH_SIZE, z_size)\n","    )\n","    \n","    # 생성 이미지(구분자의 학습에 이용)\n","    g_pred = generator.predict(z_d)\n","    \n","    # 생성자를 1스텝 학습시킨다\n","    generator.train_on_batch(z_g, batch)\n","    # discriminator 1스텝 학습시킨다\n","    _, real_loss, fake_loss = discriminator_trainer.train_on_batch(\n","            [batch, g_pred],\n","            [batch, g_pred]\n","    )\n","\n","    # k 를 갱신\n","    k_var += kLambda*(GAMMA*real_loss - fake_loss)\n","    K.set_value(k, k_var)\n","    \n","\n","    # g_measure 을 계산하기 위한 loss 저장\n","    history.append({\n","        'real_loss': real_loss,\n","        'fake_loss': fake_loss\n","    })\n","\n","    # 10번에 1번씩 로그 표시\n","    if step%10 == 0:\n","        # 과거 10 번의 measure 의 평균\n","        measurement = np.mean([\n","            measure(\n","                loss['real_loss'],\n","                loss['fake_loss'],\n","                GAMMA\n","            )\n","            for loss in history[-10:]\n","        ])\n","        \n","        logs.append({\n","            'k': K.get_value(k),\n","            'measure': measurement,\n","            'real_loss': real_loss,\n","            'fake_loss': fake_loss\n","        })\n","        print(logs[-1])\n","\n","        # 생성된 이미지 저장  \n","        img_path = '{}/generated_{}.png'.format(\n","            IMG_SAVE_DIR,\n","            step\n","        )\n","        save_imgs(\n","            img_path,\n","            generator.predict(sample_seeds),\n","            rows=IMG_SAMPLE_SHAPE[0],\n","            cols=IMG_SAMPLE_SHAPE[1]\n","        )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'k': 0.003983175, 'measure': 0.12288692034780979, 'real_loss': 0.092438966, 'fake_loss': 0.015771529}\n","{'k': 0.004277925, 'measure': 0.11872204076498746, 'real_loss': 0.0946477, 'fake_loss': 0.01480482}\n","{'k': 0.0045639463, 'measure': 0.11710310382768511, 'real_loss': 0.07971868, 'fake_loss': 0.0154863475}\n","{'k': 0.0048392536, 'measure': 0.11399490740150213, 'real_loss': 0.08482627, 'fake_loss': 0.014854597}\n","{'k': 0.005109405, 'measure': 0.11216168738901615, 'real_loss': 0.08408209, 'fake_loss': 0.015749853}\n","{'k': 0.0053801867, 'measure': 0.11207628389820457, 'real_loss': 0.08743147, 'fake_loss': 0.014991236}\n","{'k': 0.0056572063, 'measure': 0.11324446573853493, 'real_loss': 0.091282964, 'fake_loss': 0.015172268}\n","{'k': 0.0058955783, 'measure': 0.10144953848794103, 'real_loss': 0.07915608, 'fake_loss': 0.014460765}\n","{'k': 0.006164985, 'measure': 0.11083908881992102, 'real_loss': 0.08703304, 'fake_loss': 0.01531306}\n","{'k': 0.0064162053, 'measure': 0.10465782834216952, 'real_loss': 0.0782559, 'fake_loss': 0.015010981}\n","{'k': 0.0066632587, 'measure': 0.10402071680873633, 'real_loss': 0.086044945, 'fake_loss': 0.014653611}\n","{'k': 0.00690248, 'measure': 0.10213437350466847, 'real_loss': 0.07246406, 'fake_loss': 0.01570328}\n","{'k': 0.007133423, 'measure': 0.09969789450988173, 'real_loss': 0.076076746, 'fake_loss': 0.014785489}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VC8lpmqqrST-"},"source":[""]}]} 
