{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEGAN을 이용한 이미지생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'BEGAN'...\n",
      "Username for 'https://github.com': ^C\n"
     ]
    }
   ],
   "source": [
    "!git clone 'http://github.com/smartdesign/BEGAN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****라이브러리 읽어들이기****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 71,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1388,
     "status": "ok",
     "timestamp": 1516713481299,
     "user": {
      "displayName": "Mitsuhisa Ohta",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "107586005588721640993"
     },
     "user_tz": -540
    },
    "id": "6EoENk-rlqG-",
    "outputId": "0492c79c-6897-453d-8015-b00afd1583da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras import losses\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Conv2D, Conv2DTranspose, Activation, Flatten, Dense, UpSampling2D, Reshape, Lambda, Input\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.preprocessing.image import img_to_array, array_to_img\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이미지를 저장하는 함수**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(path, imgs, rows, cols):\n",
    "    \"\"\"이미지를 타일 형태로 저장\n",
    "    \n",
    "    Arguments:\n",
    "        path (str): 저장할 폴더 경로\n",
    "        imgs (np.array): 저장할 이미지 리스트\n",
    "        rows (int): 타일의 세로 크기\n",
    "        cols (int): 타일의 가로 크기\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    base_width = imgs.shape[1]\n",
    "    base_height = imgs.shape[2]\n",
    "    channels = imgs.shape[3]\n",
    "    output_shape = (\n",
    "        base_height*rows,\n",
    "        base_width*cols,\n",
    "        channels\n",
    "    )\n",
    "    buffer = np.zeros(output_shape)\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            img = imgs[row*cols + col]\n",
    "            buffer[\n",
    "                row*base_height:(row + 1)*base_height,\n",
    "                col*base_width:(col + 1)*base_width\n",
    "            ] = img\n",
    "    array_to_img(buffer).save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZhh5kkDFbXK"
   },
   "source": [
    "**이미지 데이터 읽어 들이기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 820,
     "status": "ok",
     "timestamp": 1516713490057,
     "user": {
      "displayName": "Mitsuhisa Ohta",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "107586005588721640993"
     },
     "user_tz": -540
    },
    "id": "_8jJdMsfmFQx",
    "outputId": "396afe0c-b66c-4559-ed55-c24ad3b316f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 600 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/'\n",
    "BATCH_SIZE = 24\n",
    "IMG_SHAPE = (28, 28, 3)\n",
    "\n",
    "data_gen = ImageDataGenerator(rescale=1/255.)\n",
    "train_data_generator = data_gen.flow_from_directory(\n",
    "    directory=DATA_DIR,\n",
    "    classes=['mnist'],\n",
    "    class_mode=None,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=IMG_SHAPE[:2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sHbFoscwFiPI"
   },
   "source": [
    "**Encoder 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LLFOMimZmKR_"
   },
   "outputs": [],
   "source": [
    "def build_encoder(input_shape, z_size, n_filters, n_layers):\n",
    "    \"\"\"Encoder구축\n",
    "    \n",
    "    Arguments:\n",
    "        input_shape (int): 이미지의 shape\n",
    "        z_size (int): 특징 공간의 차원 수\n",
    "        n_filters (int): 파일 수\n",
    "        \n",
    "    Returns:\n",
    "        model (Model): 인코더 모델 \n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Conv2D(\n",
    "            n_filters,\n",
    "            3,\n",
    "            activation='elu',\n",
    "            input_shape=input_shape,\n",
    "            padding='same'\n",
    "        )\n",
    "    )\n",
    "    model.add(Conv2D(n_filters, 3, padding='same'))\n",
    "    for i in range(2, n_layers + 1):\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                i*n_filters,\n",
    "                3,\n",
    "                activation='elu',\n",
    "                padding='same'\n",
    "            )\n",
    "        )\n",
    "        model.add(\n",
    "                Conv2D(\n",
    "                i*n_filters,\n",
    "                3,\n",
    "                activation='elu',\n",
    "                strides=2,\n",
    "                padding='same'\n",
    "            )\n",
    "        )\n",
    "    model.add(Conv2D(n_layers*n_filters, 3, padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(z_size))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**생성자/Decoder 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "elVBPfr-mONR"
   },
   "outputs": [],
   "source": [
    "def build_decoder(output_shape, z_size, n_filters, n_layers):\n",
    "    \"\"\"Decoder 구축\n",
    "    \n",
    "    Arguments:\n",
    "        output_shape (np.array): 이미지 shape\n",
    "        z_size (int): 특징 공간의 차원 수\n",
    "        n_filters (int): 파일 수\n",
    "        n_layers (int): 레이어 수\n",
    "        \n",
    "    Returns:\n",
    "        model (Model): 디코더 모델 \n",
    "    \"\"\"\n",
    "    # UpSampling2D로 몇 배로 확대할지 계산\n",
    "    scale = 2**(n_layers - 1)\n",
    "    # 합성곱층의 처음 입력 사이즈를 scale로부터 역산\n",
    "    fc_shape = (\n",
    "        output_shape[0]//scale,\n",
    "        output_shape[1]//scale,\n",
    "        n_filters\n",
    "    )\n",
    "    # 완전연결 계층에서 필요한 사이즈를 역산\n",
    "    fc_size = fc_shape[0]*fc_shape[1]*fc_shape[2]\n",
    "    \n",
    "    model = Sequential()\n",
    "    # 완전연결 계층\n",
    "    model.add(Dense(fc_size, input_shape=(z_size,)))\n",
    "    model.add(Reshape(fc_shape))\n",
    "    \n",
    "    # 합성곱층 반복\n",
    "    for i in range(n_layers - 1):\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                n_filters,\n",
    "                3,\n",
    "                activation='elu',\n",
    "                padding='same'\n",
    "            )\n",
    "        )\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                n_filters,\n",
    "                3,\n",
    "                activation='elu',\n",
    "                padding='same'\n",
    "            )\n",
    "        )\n",
    "        model.add(UpSampling2D())\n",
    "        \n",
    "    # 마지막 층은 UpSampling2D가 불필요\n",
    "    model.add(\n",
    "        Conv2D(\n",
    "            n_filters,\n",
    "            3,\n",
    "            activation='elu',\n",
    "            padding='same'\n",
    "        )\n",
    "    )\n",
    "    model.add(\n",
    "        Conv2D(\n",
    "            n_filters,\n",
    "            3,\n",
    "            activation='elu',\n",
    "            padding='same'\n",
    "        )\n",
    "    )\n",
    "    # 출력층에서는 3채널로\n",
    "    model.add(Conv2D(3, 3, padding='same'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**생성자 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XoubtmhlmQh3"
   },
   "outputs": [],
   "source": [
    "def build_generator(img_shape, z_size, n_filters, n_layers):\n",
    "    decoder = build_decoder(\n",
    "        img_shape, z_size, n_filters, n_layers\n",
    "    )\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**구분자 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IHMwgdgYmSwO"
   },
   "outputs": [],
   "source": [
    "def build_discriminator(img_shape, z_size, n_filters, n_layers):\n",
    "    encoder = build_encoder(\n",
    "        img_shape, z_size, n_filters, n_layers\n",
    "    )\n",
    "    decoder = build_decoder(\n",
    "        img_shape, z_size, n_filters, n_layers\n",
    "    )\n",
    "    return keras.models.Sequential((encoder, decoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**구분자의 학습용 네트워크**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RbEZL3qtmU5a"
   },
   "outputs": [],
   "source": [
    "def build_discriminator_trainer(discriminator):\n",
    "    img_shape = discriminator.input_shape[1:]\n",
    "    real_inputs = Input(img_shape)\n",
    "    fake_inputs = Input(img_shape)\n",
    "    real_outputs = discriminator(real_inputs)\n",
    "    fake_outputs = discriminator(fake_inputs)\n",
    "\n",
    "    return Model(\n",
    "        inputs=[real_inputs, fake_inputs],\n",
    "        outputs=[real_outputs, fake_outputs]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**네트워크 구축**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 731,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 915,
     "status": "ok",
     "timestamp": 1516713509294,
     "user": {
      "displayName": "Mitsuhisa Ohta",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "107586005588721640993"
     },
     "user_tz": -540
    },
    "id": "KYSTnemdmXJL",
    "outputId": "50373191-3bb8-4ee3-b587-6dd979b042b2",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 3136)              103488    \n",
      "_________________________________________________________________\n",
      "reshape_21 (Reshape)         (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_289 (Conv2D)          (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv2d_290 (Conv2D)          (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_67 (UpSampling (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_291 (Conv2D)          (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_292 (Conv2D)          (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_68 (UpSampling (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_293 (Conv2D)          (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_294 (Conv2D)          (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_295 (Conv2D)          (None, 28, 28, 3)         1731      \n",
      "=================================================================\n",
      "Total params: 326,787\n",
      "Trainable params: 326,787\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_33 (Dense)             (None, 3136)              103488    \n",
      "_________________________________________________________________\n",
      "reshape_22 (Reshape)         (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_303 (Conv2D)          (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv2d_304 (Conv2D)          (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_69 (UpSampling (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_305 (Conv2D)          (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_306 (Conv2D)          (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_70 (UpSampling (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_307 (Conv2D)          (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_308 (Conv2D)          (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_309 (Conv2D)          (None, 28, 28, 3)         1731      \n",
      "=================================================================\n",
      "Total params: 326,787\n",
      "Trainable params: 326,787\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_filters = 64  #  필터 수\n",
    "n_layers = 3 # 레이어 수\n",
    "z_size = 32  #  특징 공간의 차원\n",
    "\n",
    "generator = build_generator(\n",
    "    IMG_SHAPE, z_size, n_filters, n_layers\n",
    ")\n",
    "discriminator = build_discriminator(\n",
    "    IMG_SHAPE, z_size, n_filters, n_layers\n",
    ")\n",
    "discriminator_trainer = build_discriminator_trainer(\n",
    "    discriminator\n",
    ")\n",
    "\n",
    "generator.summary()\n",
    "# discriminator.layers[1]은 디코더를 나타냄\n",
    "discriminator.layers[1].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**손실 함수 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-i1VQ67WmeFz"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.losses import mean_absolute_error\n",
    "\n",
    "\n",
    "def build_generator_loss(discriminator):\n",
    "    # discriminator를 사용해서 손실 함수 정의\n",
    "    def loss(y_true, y_pred):\n",
    "        # y_true \n",
    "        reconst = discriminator(y_pred)\n",
    "        return mean_absolute_error(\n",
    "            reconst,\n",
    "            y_pred\n",
    "        )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generator 컴파일**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8xlrAPpOmhAf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 초기 학습률(Generator)\n",
    "g_lr = 0.0001\n",
    "\n",
    "generator_loss = build_generator_loss(discriminator)\n",
    "generator.compile(\n",
    "    loss=generator_loss,\n",
    "    optimizer=Adam(g_lr)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**구분자 컴파일**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "FcOeTJ1qmjub"
   },
   "outputs": [],
   "source": [
    "# 초기 학습률(Discriminator)\n",
    "d_lr = 0.0001\n",
    "\n",
    "# k_var는 수치(일반 변수)\n",
    "k_var = 0.0\n",
    "# k : Keras(TensorFlow) Variable\n",
    "k = K.variable(k_var)\n",
    "discriminator_trainer.compile(\n",
    "    loss=[\n",
    "        mean_absolute_error,\n",
    "        mean_absolute_error\n",
    "    ],\n",
    "    loss_weights=[1., -k],\n",
    "    optimizer=Adam(d_lr)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**수렴 판정용 함수 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(real_loss, fake_loss, gamma):\n",
    "    return real_loss + np.abs(gamma*real_loss - fake_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_e7mjM3F4Cg"
   },
   "source": [
    "**학습 코드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 434,
     "output_extras": [
      {
       "item_id": 2
      },
      {
       "item_id": 3
      },
      {
       "item_id": 4
      }
     ]
    },
    "colab_type": "code",
    "id": "ksY4_jqFml8y",
    "outputId": "669c8c2b-c62e-4b42-f52f-e0aa03548987",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'real_loss': 0.13662066, 'measure': 0.14911802858114243, 'k': 1.2497373e-05, 'fake_loss': 0.055812955}\n",
      "{'real_loss': 0.13189244, 'measure': 0.1583772406659343, 'k': 0.00027893807, 'fake_loss': 0.030157581}\n",
      "{'real_loss': 0.12587954, 'measure': 0.1612085753253528, 'k': 0.00065578445, 'fake_loss': 0.02203772}\n",
      "{'real_loss': 0.12105998, 'measure': 0.16346880955801857, 'k': 0.0010858268, 'fake_loss': 0.018418478}\n",
      "{'real_loss': 0.12116013, 'measure': 0.16404475598800472, 'k': 0.0015087012, 'fake_loss': 0.021591743}\n",
      "{'real_loss': 0.1193009, 'measure': 0.16266508283568362, 'k': 0.0018998092, 'fake_loss': 0.018728428}\n",
      "{'real_loss': 0.105666846, 'measure': 0.1601248623833793, 'k': 0.0022728709, 'fake_loss': 0.016947733}\n",
      "{'real_loss': 0.10254094, 'measure': 0.15720514396966342, 'k': 0.0026195445, 'fake_loss': 0.018427478}\n",
      "{'real_loss': 0.104002714, 'measure': 0.1542097853473675, 'k': 0.0029276928, 'fake_loss': 0.021639531}\n",
      "{'real_loss': 0.09230929, 'measure': 0.15100816119421315, 'k': 0.0032025569, 'fake_loss': 0.021239603}\n",
      "{'real_loss': 0.0907248, 'measure': 0.14867821203009918, 'k': 0.0035032462, 'fake_loss': 0.017003382}\n",
      "{'real_loss': 0.08828499, 'measure': 0.14622851251414767, 'k': 0.0037889166, 'fake_loss': 0.016887719}\n",
      "{'real_loss': 0.09367225, 'measure': 0.1445517953762338, 'k': 0.004098614, 'fake_loss': 0.0164466}\n",
      "{'real_loss': 0.089403056, 'measure': 0.14280350149417195, 'k': 0.0043924437, 'fake_loss': 0.016726477}\n",
      "{'real_loss': 0.08565176, 'measure': 0.14071169282897567, 'k': 0.0046491046, 'fake_loss': 0.018788522}\n",
      "{'real_loss': 0.09318765, 'measure': 0.13911647575333813, 'k': 0.0049204286, 'fake_loss': 0.016619375}\n",
      "{'real_loss': 0.09448761, 'measure': 0.13765728878007727, 'k': 0.005202866, 'fake_loss': 0.014012672}\n",
      "{'real_loss': 0.07866431, 'measure': 0.1362928614368913, 'k': 0.0054952325, 'fake_loss': 0.012924793}\n",
      "{'real_loss': 0.09034097, 'measure': 0.13505520636102605, 'k': 0.0057895384, 'fake_loss': 0.012466565}\n",
      "{'real_loss': 0.0810594, 'measure': 0.13385040221336, 'k': 0.0060820547, 'fake_loss': 0.012211596}\n",
      "{'real_loss': 0.078185655, 'measure': 0.1328692383165309, 'k': 0.0063865995, 'fake_loss': 0.011383035}\n",
      "{'real_loss': 0.0848575, 'measure': 0.13187520283658358, 'k': 0.0066823796, 'fake_loss': 0.011652882}\n",
      "{'real_loss': 0.084729396, 'measure': 0.130884725697406, 'k': 0.006973871, 'fake_loss': 0.011147191}\n",
      "{'real_loss': 0.08291146, 'measure': 0.1299693946045263, 'k': 0.007269198, 'fake_loss': 0.010475312}\n",
      "{'real_loss': 0.082263924, 'measure': 0.1291378052225066, 'k': 0.007563924, 'fake_loss': 0.010580826}\n",
      "{'real_loss': 0.08547139, 'measure': 0.1284062698208894, 'k': 0.007862763, 'fake_loss': 0.011644666}\n",
      "{'real_loss': 0.07853369, 'measure': 0.12759469166198226, 'k': 0.008146223, 'fake_loss': 0.0110435905}\n",
      "{'real_loss': 0.078293614, 'measure': 0.12678574043157687, 'k': 0.008422717, 'fake_loss': 0.010522109}\n",
      "{'real_loss': 0.074447274, 'measure': 0.12598231361575715, 'k': 0.008699398, 'fake_loss': 0.01020874}\n",
      "{'real_loss': 0.07716102, 'measure': 0.12537589133157045, 'k': 0.008992286, 'fake_loss': 0.010660495}\n",
      "{'real_loss': 0.078595825, 'measure': 0.12469486844883705, 'k': 0.009276055, 'fake_loss': 0.010035024}\n",
      "{'real_loss': 0.07463786, 'measure': 0.12402039313527166, 'k': 0.009552014, 'fake_loss': 0.010875518}\n",
      "{'real_loss': 0.0694048, 'measure': 0.12342044258217481, 'k': 0.0098338155, 'fake_loss': 0.009722522}\n",
      "{'real_loss': 0.071779706, 'measure': 0.12276335684667002, 'k': 0.0101076085, 'fake_loss': 0.010153936}\n",
      "{'real_loss': 0.07396305, 'measure': 0.12216769219198621, 'k': 0.01037974, 'fake_loss': 0.010471108}\n",
      "{'real_loss': 0.07386168, 'measure': 0.1215704610343078, 'k': 0.0106521845, 'fake_loss': 0.009602896}\n",
      "{'real_loss': 0.06876997, 'measure': 0.12096670221981085, 'k': 0.010920302, 'fake_loss': 0.0097661065}\n",
      "{'real_loss': 0.075146064, 'measure': 0.1204279221373427, 'k': 0.011191982, 'fake_loss': 0.010011153}\n",
      "{'real_loss': 0.07121204, 'measure': 0.11986589684026448, 'k': 0.011446578, 'fake_loss': 0.011650716}\n",
      "{'real_loss': 0.07489211, 'measure': 0.11932542837460709, 'k': 0.011700102, 'fake_loss': 0.011497821}\n",
      "{'real_loss': 0.0724327, 'measure': 0.11873249652937166, 'k': 0.011944336, 'fake_loss': 0.011671913}\n",
      "{'real_loss': 0.079404645, 'measure': 0.11824271390146583, 'k': 0.012200641, 'fake_loss': 0.011520547}\n",
      "{'real_loss': 0.069457926, 'measure': 0.11762882322249957, 'k': 0.012433756, 'fake_loss': 0.010214742}\n",
      "{'real_loss': 0.07069994, 'measure': 0.1170668331409206, 'k': 0.012673054, 'fake_loss': 0.011286229}\n",
      "{'real_loss': 0.06676177, 'measure': 0.11650648697061874, 'k': 0.012906182, 'fake_loss': 0.010729938}\n",
      "{'real_loss': 0.06689323, 'measure': 0.11595498760150304, 'k': 0.013137357, 'fake_loss': 0.011055947}\n",
      "{'real_loss': 0.061612118, 'measure': 0.11531706618189554, 'k': 0.0133518595, 'fake_loss': 0.011670076}\n",
      "{'real_loss': 0.06961133, 'measure': 0.1147176219395613, 'k': 0.013564996, 'fake_loss': 0.012066173}\n",
      "{'real_loss': 0.06192599, 'measure': 0.11406119668124372, 'k': 0.0137553755, 'fake_loss': 0.012469105}\n",
      "{'real_loss': 0.06700528, 'measure': 0.11340091875403391, 'k': 0.013936993, 'fake_loss': 0.01286443}\n",
      "{'real_loss': 0.059060812, 'measure': 0.11276457967894699, 'k': 0.014122015, 'fake_loss': 0.012994066}\n",
      "{'real_loss': 0.064161785, 'measure': 0.1121302343715427, 'k': 0.014301821, 'fake_loss': 0.013594236}\n",
      "{'real_loss': 0.058838923, 'measure': 0.11141799968594038, 'k': 0.014464004, 'fake_loss': 0.013871404}\n",
      "{'real_loss': 0.06481185, 'measure': 0.11070493769019898, 'k': 0.014616902, 'fake_loss': 0.0154877305}\n",
      "{'real_loss': 0.0587185, 'measure': 0.11000988821287307, 'k': 0.01476273, 'fake_loss': 0.014325064}\n",
      "{'real_loss': 0.057007838, 'measure': 0.10931865503494298, 'k': 0.014903909, 'fake_loss': 0.014432069}\n",
      "{'real_loss': 0.052413244, 'measure': 0.10858573249429995, 'k': 0.015030115, 'fake_loss': 0.014701863}\n",
      "{'real_loss': 0.05167134, 'measure': 0.10789536816376259, 'k': 0.015159805, 'fake_loss': 0.01484513}\n",
      "{'real_loss': 0.052740593, 'measure': 0.10720351034464401, 'k': 0.015282584, 'fake_loss': 0.016248986}\n",
      "{'real_loss': 0.053495597, 'measure': 0.10653148372949128, 'k': 0.01540627, 'fake_loss': 0.0147071555}\n",
      "{'real_loss': 0.052027065, 'measure': 0.10583964820653696, 'k': 0.015527598, 'fake_loss': 0.013949856}\n",
      "{'real_loss': 0.056763638, 'measure': 0.10521023342943767, 'k': 0.015664367, 'fake_loss': 0.012968905}\n",
      "{'real_loss': 0.053221043, 'measure': 0.10459482461520007, 'k': 0.015808916, 'fake_loss': 0.011240572}\n",
      "{'real_loss': 0.050824348, 'measure': 0.10397717676519545, 'k': 0.015958495, 'fake_loss': 0.010133042}\n",
      "{'real_loss': 0.05571417, 'measure': 0.10342623287733417, 'k': 0.016118987, 'fake_loss': 0.01219916}\n",
      "{'real_loss': 0.044759292, 'measure': 0.10285849857091126, 'k': 0.01627661, 'fake_loss': 0.008430353}\n",
      "{'real_loss': 0.051952958, 'measure': 0.10231556152747366, 'k': 0.016434433, 'fake_loss': 0.010215943}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-232e60f19a6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m     _, real_loss, fake_loss = discriminator_trainer.train_on_batch(\n\u001b[1;32m     45\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     )\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1862\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2552\u001b[0m     updated = session.run(\n\u001b[0;32m-> 2553\u001b[0;31m         fetches=fetches, feed_dict=feed_dict, **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2554\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# k의 갱신에 이용할 파라미터\n",
    "GAMMA = 0.5\n",
    "LR_K = 0.001\n",
    "\n",
    "# 반복 수. 100000～1000000 정도로 지정\n",
    "TOTAL_STEPS = 100\n",
    "\n",
    "# 모델과 확인용 생성 이미지를 저장할 폴더\n",
    "\n",
    "IMG_SAVE_DIR = 'imgs'\n",
    "# 확인용으로 4x4 개의 이미지를 생성\n",
    "IMG_SAMPLE_SHAPE = (4, 4)\n",
    "N_IMG_SAMPLES = np.prod(IMG_SAMPLE_SHAPE)\n",
    "\n",
    "\n",
    "# 저장할 폴더가 없다면 생성\n",
    "os.makedirs(IMG_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# 샘플이미지용 랜덤 시드\n",
    "sample_seeds = np.random.uniform(\n",
    "    -1, 1, (N_IMG_SAMPLES, z_size)\n",
    ")\n",
    "\n",
    "history = []\n",
    "logs = []\n",
    "\n",
    "for step, batch in enumerate(train_data_generator):\n",
    "  \n",
    "\n",
    "    # 임의의 값 생성\n",
    "    z_g = np.random.uniform(\n",
    "        -1, 1, (BATCH_SIZE, z_size)\n",
    "    )\n",
    "    z_d = np.random.uniform(\n",
    "        -1, 1, (BATCH_SIZE, z_size)\n",
    "    )\n",
    "    \n",
    "    # 생성 이미지(구분자의 학습에 이용)\n",
    "    g_pred = generator.predict(z_d)\n",
    "    \n",
    "    # 생성자를 1스텝 학습시킨다\n",
    "    generator.train_on_batch(z_g, batch)\n",
    "    # 구분자를 1스텝 학습시킨다\n",
    "    _, real_loss, fake_loss = discriminator_trainer.train_on_batch(\n",
    "            [batch, g_pred],\n",
    "            [batch, g_pred]\n",
    "    )\n",
    "\n",
    "    # k 를 갱신\n",
    "    k_var += LR_K*(GAMMA*real_loss - fake_loss)\n",
    "    K.set_value(k, k_var)\n",
    "    \n",
    "\n",
    "    # g_measure 을 계산하기 위한 loss 저장\n",
    "    history.append({\n",
    "        'real_loss': real_loss,\n",
    "        'fake_loss': fake_loss\n",
    "    })\n",
    "\n",
    "    # 10번에 1번씩 로그 표시\n",
    "    if step%10 == 0:\n",
    "        # 과거 10 번의 measure 의 평균\n",
    "        measurement = np.mean([\n",
    "            measure(\n",
    "                loss['real_loss'],\n",
    "                loss['fake_loss'],\n",
    "                GAMMA\n",
    "            )\n",
    "            for loss in history[-1000:]\n",
    "        ])\n",
    "        \n",
    "        logs.append({\n",
    "            'k': K.get_value(k),\n",
    "            'measure': measurement,\n",
    "            'real_loss': real_loss,\n",
    "            'fake_loss': fake_loss\n",
    "        })\n",
    "        print(logs[-1])\n",
    "\n",
    "        # 이미지 저장  \n",
    "        img_path = '{}/generated_{}.png'.format(\n",
    "            IMG_SAVE_DIR,\n",
    "            step\n",
    "        )\n",
    "        save_imgs(\n",
    "            img_path,\n",
    "            generator.predict(sample_seeds),\n",
    "            rows=IMG_SAMPLE_SHAPE[0],\n",
    "            cols=IMG_SAMPLE_SHAPE[1]\n",
    "        )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "BEGAN.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
